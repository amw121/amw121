{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amw121/amw121/blob/main/Python_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this session, we will use Python to run the linear regression. If you prefer to use Matlab or R, it should be fine.\n",
        "\n",
        "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
        "| ----------------| ------------------------ |\n",
        "|1.0              |               250|\n",
        "|1.7              |               300|\n",
        "|2.0              |               480|\n",
        "|2.5              |              430|\n",
        "|3.0              |               630|\n",
        "|3.2              |               730|\n",
        "\n",
        "We will use the linear regression to find the hypothesis h = wx + b;"
      ],
      "metadata": {
        "id": "dj6wDlbk5y-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single Variable Linear Regression"
      ],
      "metadata": {
        "id": "3fms6QUB9faU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
      ],
      "metadata": {
        "id": "hRfebIiW1AqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])  #(size in 1000 square feet)\n",
        "y_train = np.array([250, 300, 480,  430,   630, 730,])  #(price in 1000s of dollars)"
      ],
      "metadata": {
        "id": "6X_n5jlo1BVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "\n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = x.shape[0]\n",
        "\n",
        "    cost_sum = 0\n",
        "    for i in range(m):\n",
        "        f_wb = w * x[i] + b\n",
        "        cost = (f_wb - y[i]) ** 2\n",
        "        cost_sum = cost_sum + cost\n",
        "    total_cost = (1 / (2 * m)) * cost_sum\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "VrPnRM2Fr49x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_train, y_train,'bo')"
      ],
      "metadata": {
        "id": "OnBWzu2mr9Gk",
        "outputId": "afb29a7f-db73-4452-e7cd-446afed75013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1de8f0e650>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "    Returns\n",
        "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
        "     \"\"\"\n",
        "\n",
        "    # Number of training examples\n",
        "    m = x.shape[0]\n",
        "    dj_dw = 0\n",
        "    dj_db = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb = w * x[i] + b\n",
        "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
        "        dj_db_i = f_wb - y[i]\n",
        "        dj_db += dj_db_i\n",
        "        dj_dw += dj_dw_i\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "cRxFESNWoD2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,))  : Data, m examples\n",
        "      y (ndarray (m,))  : target values\n",
        "      w_in,b_in (scalar): initial values of model parameters\n",
        "      alpha (float):     Learning rate\n",
        "      num_iters (int):   number of iterations to run gradient descent\n",
        "      cost_function:     function to call to produce cost\n",
        "      gradient_function: function to call to produce gradient\n",
        "\n",
        "    Returns:\n",
        "      w (scalar): Updated value of parameter after running gradient descent\n",
        "      b (scalar): Updated value of parameter after running gradient descent\n",
        "      J_history (List): History of cost values\n",
        "      p_history (list): History of parameters [w,b]\n",
        "      \"\"\"\n",
        "\n",
        "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    p_history = []\n",
        "    b = b_in\n",
        "    w = w_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters using gradient_function\n",
        "        dj_dw, dj_db = gradient_function(x, y, w , b)\n",
        "\n",
        "        # Update Parameters using equation (3) above\n",
        "        b = b - alpha * dj_db\n",
        "        w = w - alpha * dj_dw\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_function(x, y, w , b))\n",
        "            p_history.append([w,b])\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
        "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
        "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
        "\n",
        "    return w, b, J_history, p_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "3L9Wnazhplf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "w_init = 0\n",
        "b_init = 0\n",
        "# some gradient descent settings\n",
        "iterations = 10000\n",
        "tmp_alpha = 1.0e-2\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,\n",
        "                                                    iterations, compute_cost, compute_gradient)\n",
        "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
      ],
      "metadata": {
        "id": "k27ITOvGpmRM",
        "outputId": "504bf7ef-336b-41b8-fba0-1501aab46f11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost 1.09e+05  dj_dw: -1.170e+03, dj_db: -4.700e+02   w:  1.170e+01, b: 4.70000e+00\n",
            "Iteration 1000: Cost 1.78e+03  dj_dw: -1.060e+00, dj_db:  2.599e+00   w:  1.974e+02, b: 3.16325e+01\n",
            "Iteration 2000: Cost 1.74e+03  dj_dw: -4.357e-01, dj_db:  1.068e+00   w:  2.045e+02, b: 1.44256e+01\n",
            "Iteration 3000: Cost 1.74e+03  dj_dw: -1.790e-01, dj_db:  4.388e-01   w:  2.074e+02, b: 7.35495e+00\n",
            "Iteration 4000: Cost 1.74e+03  dj_dw: -7.357e-02, dj_db:  1.803e-01   w:  2.085e+02, b: 4.44950e+00\n",
            "Iteration 5000: Cost 1.74e+03  dj_dw: -3.023e-02, dj_db:  7.410e-02   w:  2.090e+02, b: 3.25559e+00\n",
            "Iteration 6000: Cost 1.74e+03  dj_dw: -1.242e-02, dj_db:  3.045e-02   w:  2.092e+02, b: 2.76500e+00\n",
            "Iteration 7000: Cost 1.74e+03  dj_dw: -5.104e-03, dj_db:  1.251e-02   w:  2.093e+02, b: 2.56340e+00\n",
            "Iteration 8000: Cost 1.74e+03  dj_dw: -2.097e-03, dj_db:  5.141e-03   w:  2.093e+02, b: 2.48056e+00\n",
            "Iteration 9000: Cost 1.74e+03  dj_dw: -8.619e-04, dj_db:  2.113e-03   w:  2.094e+02, b: 2.44652e+00\n",
            "(w,b) found by gradient descent: (209.3590,  2.4325)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_train, y_train,'bo')\n",
        "x = np.linspace(1,5,10)\n",
        "y = w_final*x+ b_final\n",
        "plt.plot(x,y,'r')\n",
        "plt.title('linear regression')"
      ],
      "metadata": {
        "id": "QJUnHsOEpxhe",
        "outputId": "1c5d18e3-e79a-4d44-ce10-7e1a0bfcbf18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'linear regression')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivarible Linear Regression"
      ],
      "metadata": {
        "id": "d8oV9IV89q2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this session, we will look at the same data with multiple features:\n",
        "\n",
        "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
        "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
        "| 2104            | 5                   | 1                | 45           | 460           |  \n",
        "| 1416            | 3                   | 2                | 40           | 232           |  \n",
        "| 852             | 2                   | 1                | 35           | 178           |  \n"
      ],
      "metadata": {
        "id": "0Q9yRLAm94_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]]) # each row of x represents the features of the i the house\n",
        "y_train = np.array([460, 232, 178])\n",
        "# data is stored in numpy array/matrix\n",
        "# X_train is the data matrix\n",
        "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
        "print(X_train)\n",
        "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
        "print(y_train)\n",
        "# choose the initial values w, b that are close to the opitmal value, you may choose a random value too.\n",
        "b_init = 785.1811367994083\n",
        "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
      ],
      "metadata": {
        "id": "hXffDgyi-JgE",
        "outputId": "bdf0639e-0049-432f-a3d4-66f5f343c920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
            "[[2104    5    1   45]\n",
            " [1416    3    2   40]\n",
            " [ 852    2    1   35]]\n",
            "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
            "[460 232 178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      x (ndarray): Shape (n,) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "    p = np.dot(x, w) + b\n",
        "    return p"
      ],
      "metadata": {
        "id": "UBQqBoxC_V7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a row from our training data\n",
        "x_vec = X_train[0,:]\n",
        "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
        "\n",
        "# make a prediction\n",
        "f_wb = predict(x_vec,w_init, b_init)\n",
        "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi84zvaa_d9o",
        "outputId": "42f4d4f8-b73e-44c9-def9-8f5b2046edcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
            "f_wb shape (), prediction: 459.9999976194083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_costm(X, y, w, b): # compute the multilinear cost function\n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
        "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
        "    cost = cost / (2 * m)                      #scalar\n",
        "    return cost"
      ],
      "metadata": {
        "id": "aO8ThMP-_sjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost using our pre-chosen optimal parameters.\n",
        "cost = compute_costm(X_train, y_train, w_init, b_init)\n",
        "print(f'Cost at optimal w : {cost}')"
      ],
      "metadata": {
        "id": "TIfCvuQQ_60w",
        "outputId": "661a7652-dc1c-4bf3-9395-f11ae108e4cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at optimal w : 1.5578904428966628e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradientm(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "_JCaIGJ-AAfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute and display gradient\n",
        "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
        "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
        "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
      ],
      "metadata": {
        "id": "R9vHyj20AIRJ",
        "outputId": "785a6c26-5ae7-4a1e-cb88-53cbb568f0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db at initial w,b: [ 1.60e+06  1.74e+03  5.73e+02 -2.34e+04]\n",
            "dj_dw at initial w,b: \n",
            " [1065.49  557.69  424.03 -561.67]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descentm(X, y, w_in, b_in, cost_functionm, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( cost_functionm(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "\n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "h2waN1VGAR-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to use the Gradient Descent to find the optimal w, and predict the house price."
      ],
      "metadata": {
        "id": "o5pfLkwIBd9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "# some gradient descent settings\n",
        "iterations = 1000\n",
        "alpha = 5.0e-7\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist = gradient_descentm(X_train, y_train, initial_w, initial_b,\n",
        "                                                    compute_costm, compute_gradientm,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
        "m,_ = X_train.shape\n",
        "for i in range(m):\n",
        "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
      ],
      "metadata": {
        "id": "WnvUmPIlAWMz",
        "outputId": "9c43f382-76a0-46c6-ccba-6284cef636fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost  2529.46   \n",
            "Iteration  100: Cost   695.99   \n",
            "Iteration  200: Cost   694.92   \n",
            "Iteration  300: Cost   693.86   \n",
            "Iteration  400: Cost   692.81   \n",
            "Iteration  500: Cost   691.77   \n",
            "Iteration  600: Cost   690.73   \n",
            "Iteration  700: Cost   689.71   \n",
            "Iteration  800: Cost   688.70   \n",
            "Iteration  900: Cost   687.69   \n",
            "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
            "prediction: 426.19, target value: 460\n",
            "prediction: 286.17, target value: 232\n",
            "prediction: 171.47, target value: 178\n"
          ]
        }
      ]
    }
  ]
}